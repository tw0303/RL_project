{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GridWorld_DQN.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRuD_jqW8uzo"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import random\n",
        "import pickle\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "id": "ZiHoEC4G82xM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "833b1eed-6318-4109-91cc-9bb957933bf6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd drive/MyDrive/gridworld"
      ],
      "metadata": {
        "id": "1HK0DtLq9E1T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c671bba4-afe7-4c71-e277-95d24d3036a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/gridworld\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Environment\n",
        "class Env:\n",
        "    def __init__(self, grid = (5, 5)):\n",
        "        self.gridsize = grid\n",
        "        # goal position and initial position\n",
        "        self.goal = (grid[0] - 1, grid[1] - 1)\n",
        "        self.position = [0, 0]\n",
        "        # initialize grid\n",
        "        self.grid = np.zeros((grid[0], grid[1]))\n",
        "        self.done = 0\n",
        "\n",
        "    # reset the agent\n",
        "    def reset(self):\n",
        "        self.grid = np.zeros(self.grid.shape)\n",
        "        self.position = [0, 0]\n",
        "        self.grid[self.position[0], self.position[1]] = 1\n",
        "        self.done = 0\n",
        "        return self.grid\n",
        "    \n",
        "    # action step\n",
        "    def step(self, action):\n",
        "        # action(up:0, down:1, right:2, left:3)\n",
        "        original_position = self.position.copy()\n",
        "        out_of_boundary = False\n",
        "        # take action\n",
        "        if action == 0:\n",
        "            if self.position[0] - 1 >= 0:\n",
        "                self.position[0] = self.position[0] - 1  \n",
        "            else:\n",
        "              out_of_boundary = True \n",
        "        elif action == 1:\n",
        "            if self.position[0] + 1 < self.gridsize[0]:\n",
        "                self.position[0] = self.position[0] + 1\n",
        "            else:\n",
        "              out_of_boundary = True \n",
        "        elif action == 2:\n",
        "            if self.position[1] + 1 < self.gridsize[1]:\n",
        "                self.position[1] = self.position[1] + 1    \n",
        "            else:\n",
        "              out_of_boundary = True     \n",
        "        elif action == 3:\n",
        "            if self.position[1] - 1 >= 0:\n",
        "                self.position[1] = self.position[1] - 1\n",
        "            else:\n",
        "              out_of_boundary = True \n",
        "        # check new position\n",
        "        if self.position[0] == self.gridsize[0] - 1 and self.position[1] == self.gridsize[1] - 1:\n",
        "            reward = 1\n",
        "            done = 1\n",
        "            self.position = original_position\n",
        "        elif out_of_boundary:\n",
        "            reward = 0\n",
        "            done = 1\n",
        "        else:\n",
        "            reward = 0\n",
        "            done = 0\n",
        "            self.grid[self.position[0], self.position[1]] = 1\n",
        "            self.grid[original_position[0], original_position[1]] = 0\n",
        "        \n",
        "        return self.grid.copy(), reward, done\n",
        "\n",
        "    # set agent location\n",
        "    def set_agent_loc(self, row, col):\n",
        "        orig_agent_loc = self.position.copy()\n",
        "        self.grid[orig_agent_loc[0], orig_agent_loc[1]] = 0\n",
        "        self.position = [row, col]\n",
        "        self.grid[row, col] = 1\n",
        "        return self.grid.copy()"
      ],
      "metadata": {
        "id": "hgkxOISf9crT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ER Buffer\n",
        "class ReplayMemory:\n",
        "    __slots__ = ['buffer']\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen = capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    # append new transition into RM buffer\n",
        "    def append(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, [action], [reward], next_state, [done]))\n",
        "\n",
        "    # sample a batch of transition tensors\n",
        "    def sample(self, batch_size, device):\n",
        "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
        "        state = torch.FloatTensor(np.float32(state)).to(device)\n",
        "        action = torch.LongTensor(action).to(device)\n",
        "        next_state = torch.FloatTensor(np.float32(next_state)).to(device)\n",
        "        reward = torch.FloatTensor(reward).to(device)\n",
        "        done = torch.FloatTensor(done).to(device)       \n",
        "        return state, action, reward, next_state, done"
      ],
      "metadata": {
        "id": "eCkvuWOI-Aep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        super().__init__()\n",
        "        self.num_actions = 4\n",
        "        \n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(input_shape, 80),\n",
        "            nn.Linear(80, self.num_actions)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# DQN\n",
        "class DQN:\n",
        "    def __init__(self, args, env):\n",
        "        # behavior network\n",
        "        self._behavior_net = Net(env.gridsize[0]*env.gridsize[1], 4).to(args.device)\n",
        "        if args.k > 1:\n",
        "            # averaged-DQN\n",
        "            self._target_net = [Net(env.gridsize[0]*env.gridsize[1], 4).to(args.device) for _ in range(args.k)]\n",
        "        else:\n",
        "            # vanilla DQN\n",
        "            self._target_net = Net(env.gridsize[0]*env.gridsize[1], 4).to(args.device)\n",
        "        \n",
        "        # initialize target network\n",
        "        if args.k > 1:\n",
        "            for i in range(args.k):\n",
        "                self._target_net[i].load_state_dict(self._behavior_net.state_dict())\n",
        "        else:\n",
        "            self._target_net.load_state_dict(self._behavior_net.state_dict())\n",
        "        self._optimizer = torch.optim.Adam(self._behavior_net.parameters(), lr=args.lr)\n",
        "        # memory\n",
        "        self._memory = ReplayMemory(capacity=args.capacity)\n",
        "\n",
        "        # config \n",
        "        self.device = args.device\n",
        "        self.batch_size = args.batch_size\n",
        "        self.action_space_n = 4\n",
        "        self.gamma = args.gamma\n",
        "        self.freq = args.freq\n",
        "        self.target_freq = args.target_freq\n",
        "        self.idx = 0\n",
        "        self.k = args.k\n",
        "        self.ddqn = args.ddqn\n",
        "        self.criterion = torch.nn.MSELoss()\n",
        "        self.test_q_val = []\n",
        "        self.epoch = 0\n",
        "        self.total_steps = 0\n",
        "\n",
        "    # select action\n",
        "    def select_action(self, state, epsilon, action_space):\n",
        "        # use epsilon-greedy on behavior network\n",
        "        if epsilon > random.random():\n",
        "            action = np.random.randint(4)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                q_value = self._behavior_net(torch.Tensor(state).unsqueeze(0).to(self.device))\n",
        "                action = int(torch.argmax(q_value))\n",
        "        return action\n",
        "\n",
        "    # append transition\n",
        "    def append(self, state, action, reward, next_state, done):\n",
        "        self._memory.append(state, action, reward, next_state, done)\n",
        "\n",
        "    # update networks\n",
        "    def update(self, total_steps): \n",
        "        # behavior network      \n",
        "        if total_steps % self.freq == 0:                    \n",
        "            self._update_behavior_network(self.gamma)       \n",
        "        # target network          \n",
        "        if total_steps % self.target_freq == 0:\n",
        "            self._update_target_network()\n",
        "            self.idx += 1\n",
        "\n",
        "    # update behavior network\n",
        "    def _update_behavior_network(self, gamma):\n",
        "        # sample a minibatch of transitions\n",
        "        state, action, reward, next_state, done = self._memory.sample(self.batch_size, self.device)\n",
        "        \n",
        "        # calculate q value \n",
        "        q_value = self._behavior_net(state)\n",
        "        q_value = torch.gather(q_value, dim = 1, index=action.long())\n",
        "        with torch.no_grad():\n",
        "            # if double network setting\n",
        "            if self.ddqn:\n",
        "                # calculate q_next and action_next \n",
        "                q_next = self._behavior_net(next_state)\n",
        "                action_next = torch.argmax(q_next, dim=1).unsqueeze(-1)\n",
        "                # if averaged\n",
        "                if self.k > 1:\n",
        "                    qs = torch.zeros(self.batch_size, self.action_space_n).to(self.device)\n",
        "                    for i in range(self.k):\n",
        "                        qs += self._target_net[i](next_state)\n",
        "                    qs /= self.k\n",
        "                # if vanilla\n",
        "                else:\n",
        "                    qs = self._target_net(next_state)\n",
        "                q_next = torch.gather(qs, dim=1, index=action_next)\n",
        "                q_target = reward + gamma * q_next * (1 - done)\n",
        "            # if not double network setting\n",
        "            else:\n",
        "                # if averaged\n",
        "                if self.k > 1:\n",
        "                    q_next = torch.zeros(self.batch_size, self.action_space_n).to(self.device)\n",
        "                    for i in range(self.k):\n",
        "                        q_next += self._target_net[i](next_state)\n",
        "                    q_next /= self.k\n",
        "                # if vallina\n",
        "                else:\n",
        "                    q_next = self._target_net(next_state)\n",
        "                q_next = torch.max(q_next, dim=1).values.unsqueeze(-1)\n",
        "                q_target = reward + gamma * q_next * (1 - done)\n",
        "        # loss\n",
        "        loss = self.criterion(q_value, q_target)\n",
        "        # optimize        \n",
        "        self._optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self._optimizer.step()\n",
        "\n",
        "    # update target network    \n",
        "    def _update_target_network(self):      \n",
        "        if self.k > 1:\n",
        "            self._target_net[self.idx % self.k].load_state_dict(self._behavior_net.state_dict())\n",
        "        else:\n",
        "            self._target_net.load_state_dict(self._behavior_net.state_dict())"
      ],
      "metadata": {
        "id": "f0E1UPmd-RPc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# initialize all possible transitions into buffer\n",
        "def init_buffer(env, agent):\n",
        "    n_states = env.gridsize\n",
        "    # there are row * col-1 possibible agent location\n",
        "    for row in range(n_states[0]):\n",
        "        for col in range(n_states[1]):\n",
        "            for action in range(4):\n",
        "                if row == n_states[0] - 1 and col == n_states[1] - 1:\n",
        "                    break\n",
        "                # action(up:0, down:1, right:2, left:3)\n",
        "                state = env.set_agent_loc(row, col)\n",
        "                next_state, reward, done = env.step(action)\n",
        "                agent.append(state, action, reward, next_state, done)"
      ],
      "metadata": {
        "id": "O6vBnxOJ-dAG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train(args, env, agent):\n",
        "    action_space = 4\n",
        "    total_steps = 0\n",
        "    start_epoch = 1\n",
        "    replay_initial = 0\n",
        "    epsilon_by_steps = lambda steps, replay_start_time: 1 - (1-args.eps_min) * min(replay_start_time, args.epsilon) / args.epsilon\n",
        "\n",
        "    # initialize ER buffer\n",
        "    init_buffer(env, agent)\n",
        "\n",
        "    for epoch in range(start_epoch, args.epochs + 1):\n",
        "        print('Epoch {}/{}'.format(epoch, args.epochs))\n",
        "        print('Start Training')\n",
        "        total_reward = 0\n",
        "        state = env.reset()\n",
        "        rewards = []\n",
        "        total_q = []\n",
        "\n",
        "        # 400 iterations\n",
        "        for t in range(1, args.steps + 1):\n",
        "            # select action\n",
        "            epsilon = epsilon_by_steps(total_steps, max([total_steps-replay_initial, 0]))\n",
        "            action = agent.select_action(state, epsilon, action_space)\n",
        "            # execute action\n",
        "            next_state, reward, done = env.step(action)\n",
        "            \n",
        "            agent.update(total_steps)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            total_steps += 1\n",
        "\n",
        "            # if agent is needed to be reset\n",
        "            if done:\n",
        "                state = env.reset()\n",
        "                rewards.append(total_reward)\n",
        "                total_reward = 0\n",
        "\n",
        "        test(args, env, agent, epoch)\n",
        "\n",
        "# test part\n",
        "def test(args, env, agent, epoch):\n",
        "    print('Start Testing')\n",
        "    action_space = 4\n",
        "    n_states = env.gridsize\n",
        "    total_q = 0\n",
        "    # go through all grid and get the value of optimal action\n",
        "    for row in range(n_states[0]):\n",
        "      for col in range(n_states[1]):\n",
        "        state = env.set_agent_loc(row, col)\n",
        "        state = torch.Tensor(state).unsqueeze(0).to(args.device)\n",
        "        q = torch.zeros(1, 4).to(args.device)\n",
        "        # if averaged\n",
        "        if args.k > 1:\n",
        "            for i in range(args.k):\n",
        "                q += agent._target_net[i](state)\n",
        "            q /= args.k\n",
        "        # if vanilla\n",
        "        else:\n",
        "            q = agent._target_net(state)\n",
        "        q = torch.max(q, dim=1).values.unsqueeze(-1)\n",
        "        total_q += q.item()\n",
        "    # calculate mean\n",
        "    mean_q_val = total_q / 400\n",
        "    agent.test_q_val.append(mean_q_val)\n",
        "    print('Average Q value: {:.4f}'.format(mean_q_val))"
      ],
      "metadata": {
        "id": "9N2haTg_-iwS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Args:\n",
        "  def __init__(self, k = 1, lr = 0.0025, freq = 4, batch_size = 32, ddqn = False, result_path = 'result'):\n",
        "    self.k = k\n",
        "    self.device = \"cuda\"\n",
        "    self.size = 20\n",
        "    self.epochs = 500\n",
        "    self.steps = freq * 100\n",
        "    self.capacity = 400\n",
        "    self.batch_size = batch_size\n",
        "    self.lr = lr\n",
        "    self.epsilon = 1000\n",
        "    self.eps_min = 0.1\n",
        "    self.gamma = 0.9\n",
        "    self.freq = freq\n",
        "    self.ddqn = ddqn\n",
        "    self.target_freq = freq * 100\n",
        "    self.result_path = result_path"
      ],
      "metadata": {
        "id": "qyHY3Ev6-2-D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Vanilla DQN\n",
        "start_time = time.time()\n",
        "dir = \"DQN\"\n",
        "for i in range(1, 41):\n",
        "  print(\"{}-th trial\".format(i))\n",
        "  args = Args(k = 1, freq = 4, lr = 0.002, ddqn = False, result_path = dir + \"/result_try\")\n",
        "  env = Env((args.size, args.size))\n",
        "  args.capacity = (args.size*args.size-1) * 4\n",
        "\n",
        "  agent = DQN(args, env)\n",
        "  train(args, env, agent)\n",
        "  np.save(args.result_path + '/test_q_{}.npy'.format(i), agent.test_q_val)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "id": "nDHVLTj1_o4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Double DQN\n",
        "start_time = time.time()\n",
        "dir = \"DDQN\"\n",
        "for i in range(1, 41):\n",
        "  print(\"{}-th trial\".format(i))\n",
        "  args = Args(k = 1, freq = 4, lr = 0.002, ddqn = True, result_path = dir + \"/result_try\")\n",
        "  env = Env((args.size, args.size))\n",
        "  args.capacity = (args.size*args.size-1) * 4\n",
        "\n",
        "  agent = DQN(args, env)\n",
        "  train(args, env, agent)\n",
        "  np.save(args.result_path + '/test_q_{}.npy'.format(i), agent.test_q_val)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "id": "5VpDUvRScGls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Averaged-DQN with K = 5\n",
        "start_time = time.time()\n",
        "dir = \"AVERAGE-DQN\"\n",
        "for i in range(1, 41):\n",
        "  print(\"{}-th trial\".format(i))\n",
        "  args = Args(k = 5, freq = 4, lr = 0.002, ddqn = False, result_path = dir + \"/result/5_try\")\n",
        "  env = Env((args.size, args.size))\n",
        "  args.capacity = (args.size*args.size-1) * 4\n",
        "\n",
        "  agent = DQN(args, env)\n",
        "  train(args, env, agent)\n",
        "  np.save(args.result_path + '/test_q_{}.npy'.format(i), agent.test_q_val)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "id": "mcoFCfxMFAMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Averaged-DQN with K = 10\n",
        "start_time = time.time()\n",
        "dir = \"AVERAGE-DQN\"\n",
        "for i in range(1, 41):\n",
        "  print(\"{}-th trial\".format(i))\n",
        "  args = Args(k = 10, freq = 4, lr = 0.002, ddqn = False, result_path = dir + \"/result/10_try\")\n",
        "  env = Env((args.size, args.size))\n",
        "  args.capacity = (args.size*args.size-1) * 4\n",
        "\n",
        "  agent = DQN(args, env)\n",
        "  train(args, env, agent)\n",
        "  np.save(args.result_path + '/test_q_{}.npy'.format(i), agent.test_q_val)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "id": "jb1--2aTFYS-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Averaged-DQN with K = 20\n",
        "start_time = time.time()\n",
        "dir = \"AVERAGE-DQN\"\n",
        "for i in range(1, 41):\n",
        "  print(\"{}-th trial\".format(i))\n",
        "  args = Args(k = 20, freq = 4, lr = 0.002, ddqn = False, result_path = dir + \"/result/20_try\")\n",
        "  env = Env((args.size, args.size))\n",
        "  args.capacity = (args.size*args.size-1) * 4\n",
        "\n",
        "  agent = DQN(args, env)\n",
        "  train(args, env, agent)\n",
        "  np.save(args.result_path + '/test_q_{}.npy'.format(i), agent.test_q_val)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "id": "l9lEixZ4GGhH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Averaged-DDQN with K = 10\n",
        "start_time = time.time()\n",
        "dir = \"AVERAGE-DDQN\"\n",
        "for i in range(1, 41):\n",
        "  print(\"{}-th trial\".format(i))\n",
        "  args = Args(k = 10, freq = 4, lr = 0.002, ddqn = True, result_path = dir + \"/result/10_try\")\n",
        "  env = Env((args.size, args.size))\n",
        "  args.capacity = (args.size*args.size-1) * 4\n",
        "\n",
        "  agent = DQN(args, env)\n",
        "  train(args, env, agent)\n",
        "  np.save(args.result_path + '/test_q_{}.npy'.format(i), agent.test_q_val)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "id": "Ac-SqiXVGVFY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Averaged-DDQN with K = 20\n",
        "start_time = time.time()\n",
        "dir = \"AVERAGE-DDQN\"\n",
        "for i in range(1, 41):\n",
        "  print(\"{}-th trial\".format(i))\n",
        "  args = Args(k = 20, freq = 4, lr = 0.002, ddqn = True, result_path = dir + \"/result/20_try\")\n",
        "  env = Env((args.size, args.size))\n",
        "  args.capacity = (args.size*args.size-1) * 4\n",
        "\n",
        "  agent = DQN(args, env)\n",
        "  train(args, env, agent)\n",
        "  np.save(args.result_path + '/test_q_{}.npy'.format(i), agent.test_q_val)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "id": "9mIgws9zGd_j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}