{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Qp36fsHrrMmE"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import torch\n",
        "import random\n",
        "import pickle\n",
        "import time\n",
        "import argparse\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V8GB92VwrNFy",
        "outputId": "42195027-8a6d-41d8-b8b1-88c03d3023c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PgZQEbSwrOA_",
        "outputId": "85cc6b11-2766-49bf-efe5-7ff179566cf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/gridworld_trap\n"
          ]
        }
      ],
      "source": [
        "cd drive/MyDrive/gridworld_trap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrP5X3OYrUfa"
      },
      "outputs": [],
      "source": [
        "# Environment\n",
        "class Env:\n",
        "    def __init__(self, grid = (5, 5)):\n",
        "        self.gridsize = grid\n",
        "        # goal position, initial position and trap positions\n",
        "        self.goal = [grid[0] - 1, grid[1] - 1]\n",
        "        self.position = [0, 0]\n",
        "        self.traps = [[0, 19], [19, 0], [4, 4], [15, 15], [2, 8], [17, 11], [6, 10], [13, 9], [8, 6], [11, 13]]\n",
        "        # initialize grid\n",
        "        self.grid = np.zeros((grid[0], grid[1]))\n",
        "        self.done = 0\n",
        "\n",
        "    # reset the agent\n",
        "    def reset(self):\n",
        "        self.grid = np.zeros(self.grid.shape)\n",
        "        self.position = [0, 0]\n",
        "        self.grid[self.position[0], self.position[1]] = 1\n",
        "        self.done = 0\n",
        "        return self.grid\n",
        "    \n",
        "    # action step\n",
        "    def step(self, action, test=False):\n",
        "        # action(up:0, down:1, right:2, left:3)\n",
        "        original_position = self.position.copy()\n",
        "        out_of_boundary = False\n",
        "        # take action\n",
        "        if action == 0:\n",
        "            if self.position[0] - 1 >= 0:\n",
        "                self.position[0] = self.position[0] - 1  \n",
        "            else:\n",
        "              out_of_boundary = True \n",
        "        elif action == 1:\n",
        "            if self.position[0] + 1 < self.gridsize[0]:\n",
        "                self.position[0] = self.position[0] + 1\n",
        "            else:\n",
        "              out_of_boundary = True \n",
        "        elif action == 2:\n",
        "            if self.position[1] + 1 < self.gridsize[1]:\n",
        "                self.position[1] = self.position[1] + 1  \n",
        "            else:\n",
        "              out_of_boundary = True    \n",
        "        elif action == 3:\n",
        "            if self.position[1] - 1 >= 0:\n",
        "                self.position[1] = self.position[1] - 1\n",
        "            else:\n",
        "              out_of_boundary = True\n",
        "        # check new position\n",
        "        if self.position[0] == self.gridsize[0]-1 and self.position[1] == self.gridsize[1]-1:\n",
        "            reward = 1\n",
        "            done = 1\n",
        "            self.position = original_position\n",
        "        elif [self.position[0], self.position[1]] in self.traps:\n",
        "            reward = -1\n",
        "            done = 1\n",
        "            self.position = original_position\n",
        "        elif out_of_boundary:\n",
        "            reward = 0\n",
        "            done = 1\n",
        "        else:\n",
        "            reward = 0\n",
        "            done = 0\n",
        "            self.grid[self.position[0], self.position[1]] = 1\n",
        "            self.grid[original_position[0], original_position[1]] = 0\n",
        "        \n",
        "        return self.grid.copy(), reward, done\n",
        "\n",
        "    # set agent location\n",
        "    def set_agent_loc(self, row, col):\n",
        "        orig_agent_loc = self.position.copy()\n",
        "        self.grid[orig_agent_loc[0], orig_agent_loc[1]] = 0\n",
        "        self.position = [row, col]\n",
        "        self.grid[row, col] = 1\n",
        "        return self.grid.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFaL1QI8ssDK"
      },
      "outputs": [],
      "source": [
        "# ER Buffer\n",
        "class ReplayMemory:\n",
        "    __slots__ = ['buffer']\n",
        "\n",
        "    def __init__(self, capacity):\n",
        "        self.buffer = deque(maxlen = capacity)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    # append new transition into RM buffer\n",
        "    def append(self, state, action, reward, next_state, done):\n",
        "        self.buffer.append((state, [action], [reward], next_state, [done]))\n",
        "\n",
        "    # sample a batch of transition tensors\n",
        "    def sample(self, batch_size, device):\n",
        "        state, action, reward, next_state, done = zip(*random.sample(self.buffer, batch_size))\n",
        "        state = torch.FloatTensor(np.float32(state)).to(device)\n",
        "        action = torch.LongTensor(action).to(device)\n",
        "        next_state = torch.FloatTensor(np.float32(next_state)).to(device)\n",
        "        reward = torch.FloatTensor(reward).to(device)\n",
        "        done = torch.FloatTensor(done).to(device)       \n",
        "        return state, action, reward, next_state, done"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cE40B97rsu-m"
      },
      "outputs": [],
      "source": [
        "# Network\n",
        "class Net(nn.Module):\n",
        "    def __init__(self, input_shape, num_actions):\n",
        "        super().__init__()\n",
        "        self.num_actions = 4\n",
        "        \n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Flatten(),\n",
        "            nn.Linear(input_shape, 80),\n",
        "            nn.Linear(80, self.num_actions)\n",
        "        )\n",
        "        \n",
        "    def forward(self, x):\n",
        "        x = self.fc(x)\n",
        "        return x\n",
        "\n",
        "# DQN\n",
        "class DQN:\n",
        "    def __init__(self, args, env):\n",
        "        # behavior network\n",
        "        self._behavior_net = Net(env.gridsize[0]*env.gridsize[1], 4).to(args.device)\n",
        "        if args.k > 1:\n",
        "            # averaged-DQN\n",
        "            self._target_net = [Net(env.gridsize[0]*env.gridsize[1], 4).to(args.device) for _ in range(args.k)]\n",
        "        else:\n",
        "            # vanilla DQN\n",
        "            self._target_net = Net(env.gridsize[0]*env.gridsize[1], 4).to(args.device)\n",
        "        \n",
        "        # initialize target network\n",
        "        if args.k > 1:\n",
        "            for i in range(args.k):\n",
        "                self._target_net[i].load_state_dict(self._behavior_net.state_dict())\n",
        "        else:\n",
        "            self._target_net.load_state_dict(self._behavior_net.state_dict())\n",
        "        self._optimizer = torch.optim.Adam(self._behavior_net.parameters(), lr=args.lr)\n",
        "        # memory\n",
        "        self._memory = ReplayMemory(capacity=args.capacity)\n",
        "\n",
        "        # config \n",
        "        self.device = args.device\n",
        "        self.batch_size = args.batch_size\n",
        "        self.action_space_n = 4\n",
        "        self.gamma = args.gamma\n",
        "        self.freq = args.freq\n",
        "        self.target_freq = args.target_freq\n",
        "        self.idx = 0\n",
        "        self.k = args.k\n",
        "        self.ddqn = args.ddqn\n",
        "        self.criterion = torch.nn.MSELoss()\n",
        "        self.test_q_val = []\n",
        "        self.epoch = 0\n",
        "        self.total_steps = 0\n",
        "\n",
        "    # select action\n",
        "    def select_action(self, state, epsilon, action_space):\n",
        "        # use epsilon-greedy on behavior network\n",
        "        if epsilon > random.random():\n",
        "            action = np.random.randint(4)\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                q_value = self._behavior_net(torch.Tensor(state).unsqueeze(0).to(self.device))\n",
        "                action = int(torch.argmax(q_value))\n",
        "        return action\n",
        "\n",
        "    # append transition\n",
        "    def append(self, state, action, reward, next_state, done):\n",
        "        self._memory.append(state, action, reward, next_state, done)\n",
        "\n",
        "    # update networks\n",
        "    def update(self, total_steps): \n",
        "        # behavior network      \n",
        "        if total_steps % self.freq == 0:                    \n",
        "            self._update_behavior_network(self.gamma)       \n",
        "        # target network          \n",
        "        if total_steps % self.target_freq == 0:\n",
        "            self._update_target_network()\n",
        "            self.idx += 1\n",
        "\n",
        "    # update behavior network\n",
        "    def _update_behavior_network(self, gamma):\n",
        "        # sample a minibatch of transitions\n",
        "        state, action, reward, next_state, done = self._memory.sample(self.batch_size, self.device)\n",
        "        \n",
        "        # calculate q value \n",
        "        q_value = self._behavior_net(state)\n",
        "        q_value = torch.gather(q_value, dim = 1, index=action.long())\n",
        "        with torch.no_grad():\n",
        "            # if double network setting\n",
        "            if self.ddqn:\n",
        "                # calculate q_next and action_next \n",
        "                q_next = self._behavior_net(next_state)\n",
        "                action_next = torch.argmax(q_next, dim=1).unsqueeze(-1)\n",
        "                # if averaged\n",
        "                if self.k > 1:\n",
        "                    qs = torch.zeros(self.batch_size, self.action_space_n).to(self.device)\n",
        "                    for i in range(self.k):\n",
        "                        qs += self._target_net[i](next_state)\n",
        "                    qs /= self.k\n",
        "                # if vanilla\n",
        "                else:\n",
        "                    qs = self._target_net(next_state)\n",
        "                q_next = torch.gather(qs, dim=1, index=action_next)\n",
        "                q_target = reward + gamma * q_next * (1 - done)\n",
        "            # if not double network setting\n",
        "            else:\n",
        "                # if averaged\n",
        "                if self.k > 1:\n",
        "                    q_next = torch.zeros(self.batch_size, self.action_space_n).to(self.device)\n",
        "                    for i in range(self.k):\n",
        "                        q_next += self._target_net[i](next_state)\n",
        "                    q_next /= self.k\n",
        "                # if vallina\n",
        "                else:\n",
        "                    q_next = self._target_net(next_state)\n",
        "                q_next = torch.max(q_next, dim=1).values.unsqueeze(-1)\n",
        "                q_target = reward + gamma * q_next * (1 - done)\n",
        "        # loss\n",
        "        loss = self.criterion(q_value, q_target)\n",
        "        # optimize        \n",
        "        self._optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self._optimizer.step()\n",
        "\n",
        "    # update target network    \n",
        "    def _update_target_network(self):      \n",
        "        if self.k > 1:\n",
        "            self._target_net[self.idx % self.k].load_state_dict(self._behavior_net.state_dict())\n",
        "        else:\n",
        "            self._target_net.load_state_dict(self._behavior_net.state_dict())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Qq0CHsDsvK8"
      },
      "outputs": [],
      "source": [
        "# initialize all possible transitions into buffer\n",
        "def init_buffer(env, agent):\n",
        "    n_states = env.gridsize\n",
        "    # there are row * col-1 possibible agent location\n",
        "    for row in range(n_states[0]):\n",
        "        for col in range(n_states[1]):\n",
        "            for action in range(4):\n",
        "                if row == n_states[0] - 1 and col == n_states[1] - 1:\n",
        "                    break\n",
        "                # action(up:0, down:1, right:2, left:3)\n",
        "                state = env.set_agent_loc(row, col)\n",
        "                next_state, reward, done = env.step(action)\n",
        "                agent.append(state, action, reward, next_state, done)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ci8U1t16syDN"
      },
      "outputs": [],
      "source": [
        "def train(args, env, agent):\n",
        "    action_space = 4\n",
        "    total_steps = 0\n",
        "    start_epoch = 1\n",
        "    replay_initial = 0\n",
        "    epsilon_by_steps = lambda steps, replay_start_time: 1 - (1-args.eps_min) * min(replay_start_time, args.epsilon) / args.epsilon\n",
        "\n",
        "    # initialize ER buffer\n",
        "    init_buffer(env, agent)\n",
        "\n",
        "    for epoch in range(start_epoch, args.epochs + 1):\n",
        "        print('Epoch {}/{}'.format(epoch, args.epochs))\n",
        "        print('Start Training')\n",
        "        total_reward = 0\n",
        "        state = env.reset()\n",
        "        rewards = []\n",
        "        total_q = []\n",
        "\n",
        "        # 400 iterations\n",
        "        for t in range(1, args.steps + 1):\n",
        "            # select action\n",
        "            epsilon = epsilon_by_steps(total_steps, max([total_steps-replay_initial, 0]))\n",
        "            action = agent.select_action(state, epsilon, action_space)\n",
        "            # execute action\n",
        "            next_state, reward, done = env.step(action)\n",
        "            \n",
        "            agent.update(total_steps)\n",
        "\n",
        "            state = next_state\n",
        "            total_reward += reward\n",
        "            total_steps += 1\n",
        "\n",
        "            # if agent is needed to be reset\n",
        "            if done:\n",
        "                state = env.reset()\n",
        "                rewards.append(total_reward)\n",
        "                total_reward = 0\n",
        "\n",
        "        test(args, env, agent, epoch)\n",
        "\n",
        "# test part\n",
        "def test(args, env, agent, epoch):\n",
        "    print('Start Testing')\n",
        "    action_space = 4\n",
        "    n_states = env.gridsize\n",
        "    total_q = 0\n",
        "    # go through all grid and get the value of optimal action\n",
        "    for row in range(n_states[0]):\n",
        "      for col in range(n_states[1]):\n",
        "        state = env.set_agent_loc(row, col)\n",
        "        state = torch.Tensor(state).unsqueeze(0).to(args.device)\n",
        "        q = torch.zeros(1, 4).to(args.device)\n",
        "        # if averaged\n",
        "        if args.k > 1:\n",
        "            for i in range(args.k):\n",
        "                q += agent._target_net[i](state)\n",
        "            q /= args.k\n",
        "        # if vanilla\n",
        "        else:\n",
        "            q = agent._target_net(state)\n",
        "        q = torch.max(q, dim=1).values.unsqueeze(-1)\n",
        "        total_q += q.item()\n",
        "    # calculate mean\n",
        "    mean_q_val = total_q / 400\n",
        "    agent.test_q_val.append(mean_q_val)\n",
        "    print('Average Q value: {:.4f}'.format(mean_q_val))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fjpmU1mEszYs"
      },
      "outputs": [],
      "source": [
        "class Args:\n",
        "  def __init__(self, k = 1, lr = 0.0025, freq = 4, batch_size = 32, ddqn = False, result_path = 'result'):\n",
        "    self.k = k\n",
        "    self.device = \"cuda\"\n",
        "    self.size = 20\n",
        "    self.epochs = 500\n",
        "    self.steps = freq * 100\n",
        "    self.capacity = 400\n",
        "    self.batch_size = batch_size\n",
        "    self.lr = lr\n",
        "    self.epsilon = 1000\n",
        "    self.eps_min = 0.1\n",
        "    self.gamma = 0.9\n",
        "    self.freq = freq\n",
        "    self.ddqn = ddqn\n",
        "    self.target_freq = freq * 100\n",
        "    self.result_path = result_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTF2otFTs1AZ"
      },
      "outputs": [],
      "source": [
        "# Vanilla DQN\n",
        "start_time = time.time()\n",
        "dir = \"DQN_try\"\n",
        "for i in range(1, 41):\n",
        "  print(\"{}-th trial\".format(i))\n",
        "  args = Args(k = 1, freq = 4, lr = 0.002, ddqn = False, result_path = dir + \"/result\")\n",
        "  env = Env((args.size, args.size))\n",
        "  args.capacity = (args.size*args.size-1) * 4\n",
        "\n",
        "  agent = DQN(args, env)\n",
        "  train(args, env, agent)\n",
        "  np.save(args.result_path + '/test_q_{}.npy'.format(i), agent.test_q_val)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Double DQN\n",
        "start_time = time.time()\n",
        "dir = \"DDQN_try\"\n",
        "for i in range(1, 41):\n",
        "  print(\"{}-th trial\".format(i))\n",
        "  args = Args(k = 1, freq = 4, lr = 0.002, ddqn = True, result_path = dir + \"/result\")\n",
        "  env = Env((args.size, args.size))\n",
        "  args.capacity = (args.size*args.size-1) * 4\n",
        "\n",
        "  agent = DQN(args, env)\n",
        "  train(args, env, agent)\n",
        "  np.save(args.result_path + '/test_q_{}.npy'.format(i), agent.test_q_val)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "id": "ipowvCzDMLwi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UPFOYx-L3qUY"
      },
      "outputs": [],
      "source": [
        "# Averaged-DQN with K = 5\n",
        "start_time = time.time()\n",
        "dir = \"AVERAGE-DQN_5_try\"\n",
        "for i in range(1, 41):\n",
        "  print(\"{}-th trial\".format(i))\n",
        "  args = Args(k = 5, freq = 4, lr = 0.002, ddqn = False, result_path = dir + \"/result\")\n",
        "  env = Env((args.size, args.size))\n",
        "  args.capacity = (args.size*args.size-1) * 4\n",
        "\n",
        "  agent = DQN(args, env)\n",
        "  train(args, env, agent)\n",
        "  np.save(args.result_path + '/test_q_{}.npy'.format(i), agent.test_q_val)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Averaged-DQN with K = 10\n",
        "start_time = time.time()\n",
        "dir = \"AVERAGE-DQN_10_try\"\n",
        "for i in range(1, 41):\n",
        "  print(\"{}-th trial\".format(i))\n",
        "  args = Args(k = 10, freq = 4, lr = 0.002, ddqn = False, result_path = dir + \"/result\")\n",
        "  env = Env((args.size, args.size))\n",
        "  args.capacity = (args.size*args.size-1) * 4\n",
        "\n",
        "  agent = DQN(args, env)\n",
        "  train(args, env, agent)\n",
        "  np.save(args.result_path + '/test_q_{}.npy'.format(i), agent.test_q_val)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "id": "mDgSDPqNORfy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Averaged-DQN with K = 20\n",
        "start_time = time.time()\n",
        "dir = \"AVERAGE-DQN_20_try\"\n",
        "for i in range(1, 41):\n",
        "  print(\"{}-th trial\".format(i))\n",
        "  args = Args(k = 20, freq = 4, lr = 0.002, ddqn = False, result_path = dir + \"/result\")\n",
        "  env = Env((args.size, args.size))\n",
        "  args.capacity = (args.size*args.size-1) * 4\n",
        "\n",
        "  agent = DQN(args, env)\n",
        "  train(args, env, agent)\n",
        "  np.save(args.result_path + '/test_q_{}.npy'.format(i), agent.test_q_val)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "id": "GvAi4tOSOT7-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Averaged-DDQN with K = 10\n",
        "start_time = time.time()\n",
        "dir = \"AVERAGE-DDQN_10_try\"\n",
        "for i in range(1, 41):\n",
        "  print(\"{}-th trial\".format(i))\n",
        "  args = Args(k = 10, freq = 4, lr = 0.002, ddqn = True, result_path = dir + \"/result\")\n",
        "  env = Env((args.size, args.size))\n",
        "  args.capacity = (args.size*args.size-1) * 4\n",
        "\n",
        "  agent = DQN(args, env)\n",
        "  train(args, env, agent)\n",
        "  np.save(args.result_path + '/test_q_{}.npy'.format(i), agent.test_q_val)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "id": "_Q_WMKvtOWWz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Averaged-DDQN with K = 20\n",
        "start_time = time.time()\n",
        "dir = \"AVERAGE-DDQN_20_try\"\n",
        "for i in range(1, 41):\n",
        "  print(\"{}-th trial\".format(i))\n",
        "  args = Args(k = 20, freq = 4, lr = 0.002, ddqn = True, result_path = dir + \"/result\")\n",
        "  env = Env((args.size, args.size))\n",
        "  args.capacity = (args.size*args.size-1) * 4\n",
        "\n",
        "  agent = DQN(args, env)\n",
        "  train(args, env, agent)\n",
        "  np.save(args.result_path + '/test_q_{}.npy'.format(i), agent.test_q_val)\n",
        "print(\"--- %s seconds ---\" % (time.time() - start_time))"
      ],
      "metadata": {
        "id": "Wt2kuCHDOZ5H"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Trap_DQN.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}